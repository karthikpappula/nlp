import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import os

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Added to fix the LookupError

def load_document(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def tokenize_document(document):
    tokens = word_tokenize(document)
    return [word.lower() for word in tokens if word.isalpha()]  # Remove punctuation and convert to lowercase

def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    return [word for word in tokens if word not in stop_words]

def find_morphology(tokens):
    fdist = FreqDist(tokens)
    return fdist.most_common(10)  # Return the 10 most common words and their frequencies

document_path = 'sample_dataset.txt' # Using the previously created sample file

# Create a dummy sample file if it doesn't exist
if not os.path.exists(document_path):
    with open(document_path, 'w', encoding='utf-8') as f:
        f.write("This is a sample document. It contains some text for testing purposes. This is just a sample.")


document = load_document(document_path)
tokens = tokenize_document(document)
tokens_without_stopwords = remove_stopwords(tokens)
morphology = find_morphology(tokens_without_stopwords)

print("Morphology of the document:")
for word, frequency in morphology:
    print(f"{word}: {frequency}")
